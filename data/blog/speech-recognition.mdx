---
title: '离线语音识别调研'
date: '2025-06-04 9:43:13'
tags: ['android', '语音', '调研']
draft: false
summary: '与机器进行语音交流，让机器明白你说什么，这是人们长期以来梦寐以求的事情。语音识别技术就是让机器通过识别和理解过程把语音信号转变为相应的文本或命令的高级技术。'
authors: ['default']
---

## 简介

机器进行语音交流，让机器明白你说什么，这是人们长期以来梦寐以求的事情。语音识别技术就是让机器通过识别和理解过程把语音信号转变为相应的文本或命令的高级技术。

语音识别技术经历了长期的发展，从传统的基于隐马尔科夫模型和高斯混合模型的方法（GMM-HMM），到深度学习发展之后神经网络替代了GMM的DNN-HMM方法，再到现在的一些端到端的方法。目前，语音识别技术已经在诸多领域取得了一定的成果。

## 传统语音识别

我们以传统的语音识别方法来介绍语音识别的流程：所谓语音识别，就是将一段语音信号转换成相对应的文本信息，系统主要包含特征提取、声学模型，语言模型以及字典与解码四大部分，为了更有效地提取特征往往还需要对所采集到的声音信号进行滤波、分帧等音频数据预处理工作，将需要分析的音频信号从原始信号中合适地提取出来；特征提取工作将声音信号从时域转换到频域，为声学模型提供合适的特征向量；声学模型中再根据声学特性计算每一个特征向量在每一个状态上的得分；而语言模型则根据语言学相关的理论，计算该声音信号对应可能词组序列的概率；最后根据已有的字典，对词组序列进行解码，得到最后可能的文本表示。

![传统语音识别流程](/static/images/speech-recognition/traditional-recognition-flow.png)

我们举一个简单的例子（只是形象表述，不是真实数据和过程）：

- 语音信号：pcm、wav文件等（我是机器人）
- 特征提取：提取特征向量[1 2 3 4 56 0 ...]
- 声学模型：[1 2 3 4 56 0]-> w o s i j i q i r n
- 字典：窝：w o；我：w o；是：s i；机：j i；器：q i；人：r n；级：j i；忍：r n；
- 语言模型：我：0.0786， 是：0.0546，我是：0.0898，机器：0.0967，机器人：0.6785；
- 输出文字：我是机器人；

## 语音识别方案

| 方案         | 说明                                                                                                                                                   | 备注                                                                                                  | 官方地址                                                                                                                                                                                                     |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| vosk         | Vosk 在 Kaldi 的基础上进行了封装和改进。它提供了更简单的 API，降低了开发门槛，让开发者能更方便地将语音识别功能集成到自己的应用程序中。                 | 通过设置grammar来配置固定的识别词，极大地限制识别器的搜索空间，从而显著提高特定词汇或指令的识别准确率 | [官网](https://alphacephei.com/vosk/)                                                                                                                                                                        |
| Sherpa-onnx  | 基于微软开发的 ONNX 运行时的开源语音处理库，旨在快速高效地实现语音到文本、文本到语音以及说话者识别等功能，且无需连接互联网即可实时处理。               | 在不同硬件平台上的通用性较好，在一些资源受限的嵌入式设备上，其性能可能会受到一定影响                  | [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx)[onnx](https://onnx.ai/)                                                                                                                                 |
| Sherpa-ncnn  | 基于腾讯NCNN运行时的开源语音处理库，专门针对移动设备和嵌入式系统进行了优化，目标是提供高性能、低延迟的推理能力。                                       | 针对移动设备和嵌入式设备进行了优化，在这些设备上的推理速度通常更快，内存占用也相对较低。              | [sherpa-ncnn](https://github.com/k2-fsa/sherpa-ncnn)[ncnn](https://github.com/Tencent/ncnn)                                                                                                                  |
| PaddleSpeech | 基于百度飞桨的语音方向的开源模型库，用于语音和音频中的各种关键任务的开发。                                                                             | 官方暂无Android端成熟部署方案，支持服务端流式语音识别部署                                             | [PP-ASR](https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/docs/source/asr/PPASR_cn.md)[PaddlePaddle](https://www.paddlepaddle.org.cn/lite/v2.12/guide/introduction.html)                            |
| DeepSpeech   | 由Mozilla开发的开源语音识别引擎,它可以在各种设备上实现离线、实时的语音转文本功能，使用TensorFlow 来简化实现。                                          | 不在维护，最近更新4年前                                                                               |                                                                                                                                                                                                              |
| SpeechBrain  | 基于PyTorch的语音处理框架，提供一系列用于语音信号处理、语音识别和语音增强等任务的工具和模型。                                                          | 官方未提供Android端部署方案，既支持从头开始训练，也支持微调预训练模型                                 | [speechbrain](https://github.com/speechbrain/speechbrain)                                                                                                                                                    |
| Wenet        | 出门问问语音团队联合西工大语音实验室开源的一款面向工业落地应用的语音识别工具包，该工具用一套简洁的方案提供了语音识别从训练到部署的一条龙服务           | 使用官方提供的模型自测识别效果一般                                                                    | [Wenet介绍](https://zhuanlan.zhihu.com/p/349586567)[深度解析和刨析](https://blog.csdn.net/weixin_52734695/article/details/142868017)[WeNet2.0:提高端到端ASR的生产力](https://zhuanlan.zhihu.com/p/503739059) |
| FunASR       | 阿里提供的基础语音识别工具包，提供多种功能，包括语音识别（ASR）、语音端点检测（VAD）、标点恢复、语言模型、说话人验证、说话人分离和多人对话语音识别等。 | 官方暂无Android端成熟部署方案支持预训练好的模型的推理与微调，支持服务端流式语音识别部署               | [官网](https://github.com/modelscope/FunASR)                                                                                                                                                                 |
| SenseVoice   | 通义千问团队开源的一款多语种语音理解模型，主要针对语音识别领域，具有高精度多语言语音识别、情感辨识和音频事件检测等能力                                 | 官方未提供Android端部署方案，支持导出 ONNX 但模型比较大不适合移动端部署                               | [官网](https://github.com/FunAudioLLM/SenseVoice/tree/main)                                                                                                                                                  |

## ONNX介绍

开放神经网络交换ONNX（Open Neural Network Exchange）是一套表示深度神经网络模型的开放格式，由微软和Facebook于2017推出，然后迅速得到了各大厂商和框架的支持。通过短短几年的发展，已经成为表示深度学习模型的实际标准。ONNX定义了一组与环境和平台无关的标准格式，为AI模型的互操作性提供了基础，使AI模型可以在不同框架和环境下交互使用。硬件和软件厂商可以基于ONNX标准优化模型性能，让所有兼容ONNX标准的框架受益。

![ONNX矩阵](/static/images/speech-recognition/onnx.png)

无论你使用什么样的训练框架来训练模型（比如TensorFlow/Pytorch/OneFlow/Paddle），你都可以在训练后将这些框架的模型统一转为ONNX存储。ONNX文件不仅存储了神经网络模型的权重，还存储了模型的结构信息、网络中各层的输入输出等一些信息。目前，ONNX主要关注在模型预测方面（inferring），将转换后的ONNX模型，转换成我们需要使用不同框架部署的类型，可以很容易的部署在兼容ONNX的运行环境中。

## NCNN介绍

ncnn 是一个为**手机端**极致优化的高性能神经网络前向计算框架，也是腾讯优图实验室成立以来的第一个开源项目。ncnn 从设计之初深刻考虑手机端的部署和使用，无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架。基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 App。

## 新一代 Kaldi 的项目矩阵

包括数据、训练到部署全链条，支持语音识别(ASR)、语音合成(TTS)、关键词检测(KWS)、话音检测(VAD)、说话人识别(Speaker identification)、语种识别(Spoken language identification) 等。

![新一代Kaldi矩阵](/static/images/speech-recognition/next-kaldi-matrix.png)

## Sherpa-onnx接口参数说明

### ASR模型运行配置说明

1. **`featConfig`**：特征配置参数，用于指定音频特征的提取方式和相关参数。例如，可能包括采样率、窗口大小、步长、特征类型（如 MFCC、梅尔频谱等）等信息，这些参数决定了如何将原始音频信号转换为模型能够处理的特征向量。
2. **`modelConfig`**：模型配置参数，主要用于指定模型的架构、路径以及一些与模型加载和运行相关的参数。比如，模型的类型（如 Transformer、LSTM 等）、模型文件的路径、模型的输入输出节点名称等，告诉 ASR 系统使用哪个模型以及如何正确加载和运行该模型。
3. **`lmConfig`**：语言模型配置参数，用于配置语言模型相关的设置。包括语言模型的类型、路径、权重等信息。语言模型用于对识别结果进行后处理，根据语言的统计规律来提高识别的准确性和合理性，例如通过计算词与词之间的概率关系来纠正可能的识别错误或选择更合理的候选词。
4. **`ctcFstDecoderConfig`**：CTC FST 解码器配置参数，CTC（Connectionist Temporal Classification）是一种用于序列标注的技术，FST（Finite State Transducer）是有限状态转换器。这个配置参数主要用于设置 CTC FST 解码器的相关参数，如解码算法的类型、束宽（beam width）等，这些参数会影响解码的速度和准确性，决定了如何从模型的输出中解码出最终的文本结果。
5. **`endpointConfig`**：端点检测配置参数，用于设置音频端点检测的相关参数。端点检测是指确定音频中语音片段的起始和结束位置，例如设置能量阈值、静音持续时间等参数，以准确地检测出语音的起止点，避免在非语音部分进行不必要的识别，提高识别效率和准确性。
6. **`enableEndpoint`**：是否启用端点检测的开关参数。如果设置为 `true`，则开启端点检测功能，按照 `endpointConfig` 中设置的参数进行音频端点的检测；如果设置为 `false`，则不进行端点检测，将整个音频作为输入进行识别。
7. **`decodingMethod`**：解码方法配置参数，指定使用哪种解码算法来从模型输出中得到最终的文本结果。常见的解码方法有贪心解码（Greedy Decoding）、束搜索解码（Beam Search Decoding）等，不同的解码方法在速度和准确性上有不同的权衡，选择合适的解码方法可以根据具体应用场景和需求来优化识别效果。
8. **`maxActivePaths`**：最大活动路径数参数，主要用于束搜索解码等算法中。在解码过程中，束搜索会同时保留多个概率较高的候选路径，`maxActivePaths` 限制了这些候选路径的最大数量。通过调整这个参数，可以控制解码过程中搜索空间的大小，从而影响解码的速度和内存占用。较大的 `maxActivePaths` 可能会提高识别准确性，但会增加计算量和内存消耗；较小的值则可以加快解码速度，但可能会丢失一些较优的候选路径，导致识别准确率下降。
9. **`hotwordsFile`**：热词文件路径参数，指定包含热词列表的文件路径。热词是指在特定应用场景中具有较高重要性或频繁出现的词汇，用户希望 ASR 系统能够更准确地识别这些词汇。热词文件通常包含一行一个热词的文本内容，系统会根据这个文件中的热词来调整识别策略，提高热词的识别准确率。
10. **`hotwordsScore`**：热词得分参数，用于设置热词的得分调整值。在识别过程中，当检测到热词时，会根据这个参数来增加热词的得分，使其更有可能被识别为最终结果。较高的 `hotwordsScore` 会使热词更容易被识别出来，但如果设置不当，可能会导致一些误识别；较低的值则可能无法充分发挥热词的作用。
11. **`ruleFsts`**：规则有限状态转换器参数，用于指定一些基于规则的有限状态转换器，这些转换器可以对识别结果进行进一步的处理和约束。例如，可以通过规则 FST 来实现一些特定的语法规则、词法规则或业务逻辑的约束，对解码结果进行调整和优化，以满足特定应用场景的需求。
12. **`ruleFars`**：可能是与规则相关的另一种参数，不过具体含义可能因 `sherpa-onnx` 的具体实现和应用场景而异。一般来说，它可能与规则的应用范围、优先级或其他相关属性有关，用于更精细地控制规则的执行和效果。
13. **`blankPenalty`**：空白惩罚参数，在 CTC 解码中，空白符号表示没有实际的语音内容。`blankPenalty` 用于设置对空白符号的惩罚力度，通过调整这个参数，可以影响模型对空白符号的预测概率，从而影响识别结果中语音片段的分割和连续性。较大的空白惩罚值会使模型更倾向于减少空白符号的出现，可能导致语音片段的分割更紧密；较小的值则可能允许更多的空白出现，对语音的连续性要求相对较低。

### featConfig参数说明

**sampleRate（采样率）**

- **定义**：采样率指的是在单位时间内对音频信号进行采样的次数，其单位通常是赫兹（Hz）。它描述了在一秒钟内从连续的音频信号中提取多少个离散的样本点。
- **作用**：在 ASR 任务中，模型需要依据输入的音频信号进行特征提取和语音识别。不同的 ASR 模型是基于特定采样率的音频数据进行训练的，因此在使用 `sherpa-onnx` 运行模型时，需要确保输入音频的采样率与模型训练时的采样率一致，这样模型才能准确地处理音频信号。
- **示例**：常见的采样率有 8000Hz、16000Hz、44100Hz 等。其中，8000Hz 常用于电话语音通信，因为它能在保证一定语音质量的前提下减少数据量；而 16000Hz 是很多 ASR 模型常用的采样率，它可以提供相对较好的语音质量和识别效果；44100Hz 则常用于音乐等高质量音频的录制。

**featureDim（特征维度）**

- **定义**：特征维度表示在对音频信号进行特征提取后，每个时间步上所得到的特征向量的长度。在 ASR 中，通常会使用各种特征提取方法（如 MFCC、Fbank 等）将音频信号转换为特征向量序列，每个特征向量包含多个维度的特征信息。
- **作用**：ASR 模型会根据这些特征向量进行语音识别，特征维度的大小会影响模型的复杂度和性能。合适的特征维度可以有效地表示音频信号的特征，帮助模型更好地学习语音模式和规律。如果特征维度设置不当，可能会导致模型过拟合或欠拟合。
- **示例**：以 Fbank 特征为例，常见的特征维度可能是 80 或 128。这意味着在每个时间步上，提取的 Fbank 特征向量包含 80 个或 128 个元素，每个元素代表一个特定的特征值，这些特征值共同描述了该时间步上音频信号的特征。

### endpointConfig参数说明

endpointConfig是用于更精确控制端点检测行为的重要参数

**1. `mustContainNonSilence`**

当设置为 `true` 时，表示一个有效的语音片段必须包含非静音部分；若设置为 `false`，即使音频片段全部为静音，也可能被视为一个有效的语音片段。

**2. `minTrailingSilence`**

它规定了在判定语音结束时，语音片段末尾的静音部分必须持续的最短时间，单位为秒。帮助更准确地识别语音的结束点。例如，人们在说话结束后可能会有短暂停顿，但只有当这个停顿达到一定时长时，才认为语音真正结束。

**3. `minUtteranceLength`**

它表示一个有效的语音片段必须达到的最小时长，单位为秒。过滤掉过短的语音片段，避免对一些无意义的短音频进行识别处理，提高识别效率。比如，背景中的一些短暂噪音可能会被误识别为语音，通过设置 `minUtteranceLength`，可以将这些短音频排除在外。

## 结论

如果是固定语音指令识别场景直接使用vosk开源方案即可，如果是开放词汇语音识别使用Sherpa-onnx方案并且寻找在性能和识别准确度上平衡的模型。

1. [5个最流行的开源ASR模型](http://www.hubwiz.com/blog/top5-open-source-asr-models/)
2. [13个最佳开源语音识别引擎](https://zhuanlan.zhihu.com/p/679165787)
3. [各安卓开源离线语音识别项目的测试结果](https://blog.csdn.net/l156789/article/details/142760609)
